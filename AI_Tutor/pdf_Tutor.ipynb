{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "purlJh2a2dfM"
   },
   "source": [
    "#**AI Tutor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bhFPcZ721ma"
   },
   "source": [
    "Requirements:                                                        \n",
    "`torch` :                                                     \n",
    "Installs PyTorch, a deep learning framework used for training and inference.\n",
    "\n",
    "Required for running SentenceTransformer and other ML models.                                                              `transformer` :    \n",
    "Used for running large language models (LLMs) like BERT, GPT, etc.     \n",
    "\n",
    "`pypdf` :               \n",
    "Python library to read and extract text from PDF files.\n",
    "\n",
    "`PyPDF2` is a Python library used to read, manipulate, and write PDF files. It's widely used in document processing workflows.\n",
    "\n",
    "`faiss-cpu` :            \n",
    "Facebook AI Similarity Search (FAISS), CPU version.\n",
    "\n",
    "Used for fast vector search and similarity matching (for embeddings).  \n",
    "\n",
    "`python-dotenv` :           \n",
    "loads environment variables from a .env file into your project. (useful for hiding API keys).              \n",
    "\n",
    "`accelerate` :        \n",
    " a library by Hugging Face that automatically handles hardware (CPU, GPU, TPU) configuration and optimization for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83317,
     "status": "ok",
     "timestamp": 1745291967599,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "ZeLRELovEbIq",
    "outputId": "c08f951c-9651-4a6d-8d55-334365809b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers sentence-transformers pypdf faiss-cpu python-dotenv accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91188,
     "status": "ok",
     "timestamp": 1745324193767,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "EekUxxAibDIB",
    "outputId": "425b13c8-48e1-4331-e679-9dbc57cfc327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=120ee61885d95f9c1b2facc2ed959d1155565fc8c40b319c301de79a28594a9d\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: PyPDF2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, langdetect, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.10.0 langdetect-1.0.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install torch faiss-cpu langchain PyPDF2 sentence-transformers langdetect google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1745324256052,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "C6Lt7c9YFc0f",
    "outputId": "da3f1496-dfac-4f6e-ba5a-9e3129f429e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5650,
     "status": "ok",
     "timestamp": 1745325885798,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "dE1S82NmhrdS",
    "outputId": "856272b4-7e07-48bb-9875-2d00f6047a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytesseract, pdf2image\n",
      "Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "E: Unable to locate package tesseract-ocr-te\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract pdf2image\n",
    "!sudo apt-get install tesseract-ocr tesseract-ocr-te  # Telugu language support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6251,
     "status": "ok",
     "timestamp": 1745326328471,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "Ifmv7HxSjiMa",
    "outputId": "8d954b80-a86b-48fa-d4a6-02a1b0fb4092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  poppler-utils\n",
      "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
      "Need to get 186 kB of archives.\n",
      "After this operation, 696 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7 [186 kB]\n",
      "Fetched 186 kB in 1s (219 kB/s)\n",
      "Selecting previously unselected package poppler-utils.\n",
      "(Reading database ... 126332 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.7_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.7) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.7) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y poppler-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745325945997,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "w_Mr0tELE4EX"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch  #  PyTorch library - used for tensor computations and model operations (e.g., if using a transformer model with GPU)\n",
    "\n",
    "import google.generativeai as genai  # Google Generative AI client - used to interact with Gemini models via Google AI Studio API\n",
    "\n",
    "from sentence_transformers import SentenceTransformer  # SentenceTransformer - used to convert text into high-quality vector embeddings for semantic search\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Langchain's splitter - breaks long text into smaller overlapping chunks (useful for context windows in LLMs)\n",
    "\n",
    "from PyPDF2 import PdfReader  # PDF reader - extracts raw text from PDF files page by page (can be replaced with `pypdf` for better support)\n",
    "\n",
    "import faiss  # Facebook AI Similarity Search - helps in fast and efficient similarity search over vector embeddings (used for retrieving relevant chunks)\n",
    "\n",
    "import numpy as np  # NumPy - numerical computing library used for handling arrays and converting embeddings to FAISS-compatible formats\n",
    "from langdetect import detect # Langdetect - used to detect the language of the text\n",
    "from langdetect.lang_detect_exception import LangDetectException # Exception handling for language detection\n",
    "from pdf2image import convert_from_path # PDF to image conversion library - used for converting PDF pages to images for OCR processing\n",
    "import pytesseract # OCR library - used to extract text from images (e.g., scanned PDF pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745324338117,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "StfbJEZQ0K_V"
   },
   "outputs": [],
   "source": [
    "# Set your Gemini API key from Google AI Studio\n",
    "genai.configure(api_key=\"AIghcjx\")  # Replace this with your actual Gemini API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1745324385544,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "wILefReXE5UG"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  #  Pretrained sentence embedding model (from SentenceTransformers) for semantic similarity search(L6 6 transformers layers)\n",
    "    CHUNK_SIZE = 1000                      #  Number of characters per text chunk when splitting the PDF text\n",
    "    CHUNK_OVERLAP = 100                    #  Overlapping characters between consecutive chunks to preserve context\n",
    "    TOP_K = 3                             #  Number of most relevant chunks to retrieve for answering a question\n",
    "    GEMINI_MODEL = \"gemini-1.5-flash\"     #  Gemini model version to be used for generating answers (Fast and efficient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Uuj3AV1-nc3"
   },
   "source": [
    "# CHUNK_OVERLAP = 50                                              \n",
    "When you split a large text (like a PDF) into smaller pieces (chunks), each chunk contains a part of the text. These chunks are arranged one after anothe  \n",
    "\n",
    "`\"AlexNet is a convolutional neural network designed by Alex Krizhevsky, which revolutionized image classification.\"`\n",
    "\n",
    "If we split the document into chunks of 50 characters without overlap, we might get:\n",
    "\n",
    "Chunk 1: \"AlexNet is a convolutional neural network designed\"\n",
    "\n",
    "Chunk 2: \" by Alex Krizhevsky, which revolutionized image cla\"\n",
    "\n",
    "Now, the sentence is broken in the middle â€” this makes it harder for the model to understand the meaning.\n",
    "\n",
    "So, we overlap the end of one chunk with the start of the next, like this:\n",
    "\n",
    "Chunk 1: \"AlexNet is a convolutional neural network designed\"\n",
    "\n",
    "Chunk 2: \"network designed by Alex Krizhevsky, which revoluti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 158119,
     "status": "ok",
     "timestamp": 1745327307446,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "NHNXLByWE-oO",
    "outputId": "f11b290c-2e06-461a-bccc-90456616a248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question (or 'exit'): à°¬à±‹à°§à±à°¨à°¯à°¶à°¾à°¸à°¾à°°à°‚\n",
      "\n",
      "ðŸ¤– Answer: The provided text does not contain a section or definition explicitly titled \"à°¬à±‹à°§à±à°¨à°¯à°¶à°¾à°¸à°¾à°°à°‚\" (Bodhana Shaasaram).  Therefore, I cannot provide a direct answer in Telugu or English.  The document discusses aspects of education reform, including teacher eligibility tests (TET), but doesn't offer a definition for that specific term.\n",
      "\n",
      "Your question (or 'exit'): à°¬à±‹à°§à±à°¨\n",
      "\n",
      "ðŸ¤– Answer: The provided text mentions \"à°¬à±‹à°§à°¨\" (bodhana) within the context of  paragraph 4 which discusses improvements needed in schools.  The exact phrase is  \"à°ªà°°à° à°¶à°°à°²à±à°²à±‹  à°ªà°°à° à°¯à°ªà°¾à°£à°˜à°³à°¿à°•, à°¬à±‹à°§à°¨à°˜ à°¶à°°à°¸à±à°°à°‚\" which translates to \"Teaching-learning process and teaching quality in schools\".\n",
      "\n",
      "\n",
      "**Telugu:** à°¬à±‹à°§à°¨ à°…à°‚à°Ÿà±‡ à°¬à±‹à°§à°¿à°‚à°šà°¡à°‚, à°‰à°ªà°¦à±‡à°¶à°¿à°‚à°šà°¡à°‚.  à°‡à°•à±à°•à°¡, à°ªà°¾à° à°¶à°¾à°²à°²à±à°²à±‹ à°¬à±‹à°§à°¨ à°ªà±à°°à°•à±à°°à°¿à°¯ à°®à°°à°¿à°¯à± à°¬à±‹à°§à°¨ à°¨à°¾à°£à±à°¯à°¤à°¨à± à°®à±†à°°à±à°—à±à°ªà°°à°šà°¡à°‚ à°—à±à°°à°¿à°‚à°šà°¿ à°šà°°à±à°šà°¿à°‚à°šà°¬à°¡à°¿à°‚à°¦à°¿.\n",
      "\n",
      "\n",
      "**English:**  \"à°¬à±‹à°§à°¨\" (bodhana) means teaching or instruction.  In this context, it refers to the teaching-learning process and the quality of teaching within schools, which are discussed as areas needing improvement.\n",
      "\n",
      "Your question (or 'exit'): à°œà°¾à°¤à±€à°¯ à°µà°¿à°¦à±à°¯à°¾à°µà°¿à°§à°¨à°‚\n",
      "\n",
      "ðŸ¤– Answer: **Telugu:**\n",
      "\n",
      "à°œà°¾à°¤à±€à°¯ à°µà°¿à°¦à±à°¯à°¾à°µà°¿à°§à°¾à°¨à°‚ 2020, 5+3+3+4 à°¨à°®à±‚à°¨à°¾à°¨à± à°…à°¨à±à°¸à°°à°¿à°¸à±à°¤à±à°‚à°¦à°¿.  à°‡à°‚à°¦à±à°²à±‹ à°¨à°¾à°²à±à°—à± à°¦à°¶à°²à± à°‰à°¨à±à°¨à°¾à°¯à°¿:  à°ªà±à°¨à°°à±à°¨à°¿à°°à±à°®à°¾à°£ à°¦à°¶ (à°«à±Œà°‚à°¡à±‡à°·à°¨à± à°¸à±à°Ÿà±‡à°œà±) (3+2 à°¸à°‚à°µà°¤à±à°¸à°°à°¾à°²à±), à°¸à°¿à°¦à±à°§à°¿à°ªà°°à°Ÿà± à°¦à°¶ (à°ªà±à°°à°¿à°ªà°°à±‡à°Ÿà°°à±€ à°¸à±à°Ÿà±‡à°œà±) (3 à°¸à°‚à°µà°¤à±à°¸à°°à°¾à°²à±), à°®à°§à±à°¯ à°¦à°¶ (à°®à°¿à°¡à°¿à°²à± à°¸à±à°Ÿà±‡à°œà±) (3 à°¸à°‚à°µà°¤à±à°¸à°°à°¾à°²à±), à°®à°°à°¿à°¯à± à°®à°¾à°§à±à°¯à°®à°¿à°• à°¦à°¶ (à°¸à±†à°•à°‚à°¡à°°à±€ à°¸à±à°Ÿà±‡à°œà±) (4 à°¸à°‚à°µà°¤à±à°¸à°°à°¾à°²à±).  à°ˆ à°µà°¿à°§à°¾à°¨à°‚ 3 à°¸à°‚à°µà°¤à±à°¸à°°à°¾à°² à°µà°¯à°¸à±à°¸à± à°¨à±à°‚à°¡à°¿ 18 à°¸à°‚à°µà°¤à±à°¸à°°à°¾à°² à°µà°¯à°¸à±à°¸à± à°µà°°à°•à± à°µà°¿à°¦à±à°¯à°¨à± à°•à°µà°°à± à°šà±‡à°¸à±à°¤à±à°‚à°¦à°¿.  à°ªà°¾à° à±à°¯à°ªà±à°°à°£à°¾à°³à°¿à°• à°®à°°à°¿à°¯à± à°¬à±‹à°§à°¨à°¾ à°¶à±ˆà°²à°¿à°²à±‹ à°•à±‚à°¡à°¾ à°®à°¾à°°à±à°ªà±à°²à± à°šà±‡à°¯à°¬à°¡à±à°¡à°¾à°¯à°¿. à°ªà±à°°à°¾à°¥à°®à°¿à°• à°¬à°¾à°²à±à°¯ à°¸à°‚à°°à°•à±à°·à°£ à°®à°°à°¿à°¯à± à°µà°¿à°¦à±à°¯à°•à± à°ªà±à°°à°¾à°§à°¾à°¨à±à°¯à°¤ à°‡à°µà±à°µà°¬à°¡à°¿à°‚à°¦à°¿.\n",
      "\n",
      "\n",
      "**English:**\n",
      "\n",
      "The National Education Policy (NEP) 2020 follows a 5+3+3+4 structure. It comprises four stages: Foundational Stage (3+2 years), Preparatory Stage (3 years), Middle Stage (3 years), and Secondary Stage (4 years).  This policy covers education from the age of 3 to 18.  Changes have been made to the curriculum and teaching methodology.  Emphasis is given to early childhood care and education.\n",
      "\n",
      "Your question (or 'exit'): exit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Main class to handle PDF reading, question answering and Gemini response generation\n",
    "class PDFQuestionAnswering:\n",
    "    def __init__(self, pdf_path: str):\n",
    "        \"\"\"Initialize the question answering system with a PDF file.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file to process\n",
    "        \"\"\"\n",
    "        self.pdf_path = pdf_path       # Stores the pdf filepath\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # it uses GPU, if not use cpu\n",
    "        self.embedder = SentenceTransformer(Config.EMBEDDING_MODEL) # Loading the embedding model\n",
    "        self.docs = self._load_pdf() # Extracts raw text from pdf\n",
    "        self.chunks = self._split_chunks() # Split text into chunks\n",
    "        # generate embeddings for each text chunk(normalized for better similarity comparison)\n",
    "        self.chunk_embeddings = self.embedder.encode(self.chunks, convert_to_tensor=False, normalize_embeddings=True)\n",
    "        self.index = self._create_faiss_index() # Create Search index for fast retreival\n",
    "        self.model = genai.GenerativeModel(Config.GEMINI_MODEL) # Intializing gemini modelfor answer generation\n",
    "\n",
    "    def _load_pdf(self) -> str:\n",
    "        \"\"\"Load the PDF file and extract raw text.\n",
    "\n",
    "        Returns:\n",
    "            str: Raw text extracted from the PDF\n",
    "        \"\"\"\n",
    "        reader = PdfReader(self.pdf_path) # read the pdf\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            content = page.extract_text() # extracting the text\n",
    "            if content: # only adding non empty pages\n",
    "                text += content + \"\\n\" # adding new line between pages\n",
    "        return text\n",
    "\n",
    "    def _split_chunks(self):\n",
    "        \"\"\"Split the text into smaller chunks for processing.\n",
    "\n",
    "        Returns:\n",
    "            list: List of text chunks\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=Config.CHUNK_SIZE,   # Number of charachter per chunksize\n",
    "            chunk_overlap=Config.CHUNK_OVERLAP # Overlap between chunks for context presentation\n",
    "        )\n",
    "        return splitter.split_text(self.docs)\n",
    "\n",
    "    def _create_faiss_index(self):\n",
    "        \"\"\"Create a FAISS index for efficient similarity search of text chunks.\n",
    "\n",
    "        Returns:\n",
    "            faiss.Index: Search index containnig chunk embeddings\n",
    "        \"\"\"\n",
    "        dim = len(self.chunk_embeddings[0]) # Get embedding dimensions\n",
    "        index = faiss.IndexFlatL2(dim) # Use L2 distance for similarity(eucli)\n",
    "        index.add(np.array(self.chunk_embeddings).astype(\"float32\")) # add embeddings to index\n",
    "        return index\n",
    "\n",
    "    def _search_similar_chunks(self, query: str):\n",
    "        \"\"\"Find most relevant text chunks for a given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): User's question/search query\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Top K most relevant text chunks\n",
    "        \"\"\"\n",
    "        # [query] is wrapped in a list because encode() expects multiple texts\n",
    "        # convert_to_tensor=False returns numpy arrays instead of PyTorch tensors\n",
    "        # normalize_embeddings=True scales vectors to unit length for better similarity comparison\n",
    "        query_embedding = self.embedder.encode([query], convert_to_tensor=False, normalize_embeddings=True)\n",
    "        # Search the FAISS index for most similar document chunks:\n",
    "        # 1. Convert query embedding to float32 numpy array (required by FAISS)\n",
    "        # 2. Search for TOP_K nearest neighbors (Config.TOP_K specifies how many)\n",
    "         # Returns:\n",
    "        # - distances: L2 distances to each neighbor (smaller = more similar)\n",
    "        # - indices: Positions of matching chunks in the original chunks list\n",
    "        distances, indices = self.index.search(np.array(query_embedding).astype(\"float32\"), Config.TOP_K)\n",
    "        # Retrieve the actual text chunks corresponding to the found indices\n",
    "        # indices[0] accesses the results for our single query (batch dimension 0)\n",
    "        # Returns a list of the top-K most relevant text passages\n",
    "        return [self.chunks[i] for i in indices[0]] # Return actual text chunks\n",
    "    def _detect_language(self, text_sample: str) -> str:\n",
    "        try:\n",
    "            return detect(text_sample)\n",
    "        except LangDetectException:\n",
    "            return \"en\"\n",
    "\n",
    "    def generate_answer(self, question: str, max_new_tokens: int = 300):\n",
    "        \"\"\"Generate an answer to a question using relevant context from the PDF.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to answer\n",
    "            max_new_tokens (int): Maximum length of generated response\n",
    "\n",
    "        Returns:\n",
    "            str: Generated answer\n",
    "        \"\"\"\n",
    "        # Step 1: Find most relevant text chunks\n",
    "        top_chunks = self._search_similar_chunks(question)\n",
    "        context = \"\\n\".join(top_chunks) # Combine chunks into single context string\n",
    "        detect_lang = self._detect_language(context[:500])\n",
    "\n",
    "        # Multilingual instruction logic\n",
    "        if detect_lang == \"te\":\n",
    "            instruction = 'Answer in both Telugu and English.'\n",
    "        elif detect_lang == \"ta\":\n",
    "            instruction = 'Answer in both Tamil and English.'\n",
    "        elif detect_lang == \"hi\":\n",
    "            instruction = 'Answer in both Hindi and English.'\n",
    "        else:\n",
    "            instruction = 'Answer in English'\n",
    "        # Step 2: Create structured prompt for Gemini\n",
    "        prompt = (\n",
    "            f\"You are a multilingual AI tutor. Based on the following context, answer the question clearly.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\" # Relavents Passages from pdf\n",
    "            f\"Question:\\n{question}\\n\\n\" # Users orginal question\n",
    "            f\"{instruction}\" # Instruction for the model\n",
    "        )\n",
    "        # Step 3: Generate and return response\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text.strip() # Clean up whitespaces in response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path of pdf\n",
    "    pdf_path = \"/content/drive/MyDrive/AI_Tutor/AI_Tutor/TELUGU.pdf\"\n",
    "    bot = PDFQuestionAnswering(pdf_path) # Initialize bot with pdf\n",
    "\n",
    "    # Interactive Q & A loop\n",
    "    while True:\n",
    "        question = input(\"\\nYour question (or 'exit'): \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        answer = bot.generate_answer(question) # Generate Answer\n",
    "        print(\"\\nðŸ¤– Answer:\", answer) # Display formatted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23667,
     "status": "ok",
     "timestamp": 1745325057906,
     "user": {
      "displayName": "Dayakar Vallepu",
      "userId": "04978987307118948295"
     },
     "user_tz": -330
    },
    "id": "8scLlA43NW7T",
    "outputId": "c37eceb3-6e54-406f-823e-7d8aca953636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
