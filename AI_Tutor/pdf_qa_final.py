# -*- coding: utf-8 -*-
"""pdf_Tutor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lEsncDJEzZrkQ7CWEENUcts58berdKWl

#**AI Tutor**

Requirements:                                                        
`torch` :                                                     
Installs PyTorch, a deep learning framework used for training and inference.

Required for running SentenceTransformer and other ML models.                                                              `transformer` :    
Used for running large language models (LLMs) like BERT, GPT, etc.     

`pypdf` :               
Python library to read and extract text from PDF files.

`PyPDF2` is a Python library used to read, manipulate, and write PDF files. It's widely used in document processing workflows.

`faiss-cpu` :            
Facebook AI Similarity Search (FAISS), CPU version.

Used for fast vector search and similarity matching (for embeddings).  

`python-dotenv` :           
loads environment variables from a .env file into your project. (useful for hiding API keys).              

`accelerate` :        
 a library by Hugging Face that automatically handles hardware (CPU, GPU, TPU) configuration and optimization for training or inference.
"""

# !pip install -q torch transformers sentence-transformers pypdf faiss-cpu python-dotenv accelerate

# !pip install torch faiss-cpu langchain PyPDF2 sentence-transformers langdetect google-generativeai

# !pip install PyPDF2

# !pip install pytesseract pdf2image
# !sudo apt-get install tesseract-ocr tesseract-ocr-te  # Telugu language support

# !apt-get install -y poppler-utils

# Import libraries
import torch  #  PyTorch library - used for tensor computations and model operations (e.g., if using a transformer model with GPU)

import google.generativeai as genai  # Google Generative AI client - used to interact with Gemini models via Google AI Studio API

from sentence_transformers import SentenceTransformer  # SentenceTransformer - used to convert text into high-quality vector embeddings for semantic search

from langchain.text_splitter import RecursiveCharacterTextSplitter  # Langchain's splitter - breaks long text into smaller overlapping chunks (useful for context windows in LLMs)

from PyPDF2 import PdfReader  # PDF reader - extracts raw text from PDF files page by page (can be replaced with `pypdf` for better support)

import faiss  # Facebook AI Similarity Search - helps in fast and efficient similarity search over vector embeddings (used for retrieving relevant chunks)

import numpy as np  # NumPy - numerical computing library used for handling arrays and converting embeddings to FAISS-compatible formats
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException
from pdf2image import convert_from_path
import pytesseract

# Set your Gemini API key from Google AI Studio
genai.configure(api_key="AIz")  # Replace this with your actual Gemini API key

# Configuration
class Config:
    EMBEDDING_MODEL = "all-MiniLM-L6-v2"  #  Pretrained sentence embedding model (from SentenceTransformers) for semantic similarity search(L6 6 transformers layers)
    CHUNK_SIZE = 1000                      #  Number of characters per text chunk when splitting the PDF text
    CHUNK_OVERLAP = 100                    #  Overlapping characters between consecutive chunks to preserve context
    TOP_K = 3                             #  Number of most relevant chunks to retrieve for answering a question
    GEMINI_MODEL = "gemini-1.5-flash"     #  Gemini model version to be used for generating answers (Fast and efficient)

"""# CHUNK_OVERLAP = 50                                              
When you split a large text (like a PDF) into smaller pieces (chunks), each chunk contains a part of the text. These chunks are arranged one after anothe  

`"AlexNet is a convolutional neural network designed by Alex Krizhevsky, which revolutionized image classification."`

If we split the document into chunks of 50 characters without overlap, we might get:

Chunk 1: "AlexNet is a convolutional neural network designed"

Chunk 2: " by Alex Krizhevsky, which revolutionized image cla"

Now, the sentence is broken in the middle â€” this makes it harder for the model to understand the meaning.

So, we overlap the end of one chunk with the start of the next, like this:

Chunk 1: "AlexNet is a convolutional neural network designed"

Chunk 2: "network designed by Alex Krizhevsky, which revoluti"
"""

# Main class to handle PDF reading, question answering and Gemini response generation
class PDFQuestionAnswering:
    def __init__(self, pdf_path: str):
        """Initialize the question answering system with a PDF file.

        Args:
            pdf_path (str): Path to the PDF file to process
        """
        self.pdf_path = pdf_path       # Stores the pdf filepath
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # it uses GPU, if not use cpu
        self.embedder = SentenceTransformer(Config.EMBEDDING_MODEL) # Loading the embedding model
        self.docs = self._load_pdf() # Extracts raw text from pdf
        self.chunks = self._split_chunks() # Split text into chunks
        # generate embeddings for each text chunk(normalized for better similarity comparison)
        self.chunk_embeddings = self.embedder.encode(self.chunks, convert_to_tensor=False, normalize_embeddings=True)
        self.index = self._create_faiss_index() # Create Search index for fast retreival
        self.model = genai.GenerativeModel(Config.GEMINI_MODEL) # Intializing gemini modelfor answer generation

    def _load_pdf(self) -> str:
        """Load the PDF file and extract raw text.

        Returns:
            str: Raw text extracted from the PDF
        """
        reader = PdfReader(self.pdf_path) # read the pdf
        text = ""
        for page in reader.pages:
            content = page.extract_text() # extracting the text
            if content: # only adding non empty pages
                text += content + "\n" # adding new line between pages
        return text

    def _split_chunks(self):
        """Split the text into smaller chunks for processing.

        Returns:
            list: List of text chunks
        """
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,   # Number of charachter per chunksize
            chunk_overlap=Config.CHUNK_OVERLAP # Overlap between chunks for context presentation
        )
        return splitter.split_text(self.docs)

    def _create_faiss_index(self):
        """Create a FAISS index for efficient similarity search of text chunks.

        Returns:
            faiss.Index: Search index containnig chunk embeddings
        """
        dim = len(self.chunk_embeddings[0]) # Get embedding dimensions
        index = faiss.IndexFlatL2(dim) # Use L2 distance for similarity(eucli)
        index.add(np.array(self.chunk_embeddings).astype("float32")) # add embeddings to index
        return index

    def _search_similar_chunks(self, query: str):
        """Find most relevant text chunks for a given query.

        Args:
            query (str): User's question/search query

        Returns:
            List[str]: Top K most relevant text chunks
        """
        # [query] is wrapped in a list because encode() expects multiple texts
        # convert_to_tensor=False returns numpy arrays instead of PyTorch tensors
        # normalize_embeddings=True scales vectors to unit length for better similarity comparison
        query_embedding = self.embedder.encode([query], convert_to_tensor=False, normalize_embeddings=True)
        # Search the FAISS index for most similar document chunks:
        # 1. Convert query embedding to float32 numpy array (required by FAISS)
        # 2. Search for TOP_K nearest neighbors (Config.TOP_K specifies how many)
         # Returns:
        # - distances: L2 distances to each neighbor (smaller = more similar)
        # - indices: Positions of matching chunks in the original chunks list
        distances, indices = self.index.search(np.array(query_embedding).astype("float32"), Config.TOP_K)
        # Retrieve the actual text chunks corresponding to the found indices
        # indices[0] accesses the results for our single query (batch dimension 0)
        # Returns a list of the top-K most relevant text passages
        return [self.chunks[i] for i in indices[0]] # Return actual text chunks
    def _detect_language(self, text_sample: str) -> str:
        try:
            return detect(text_sample)
        except LangDetectException:
            return "en"

    def generate_answer(self, question: str, max_new_tokens: int = 300):
        """Generate an answer to a question using relevant context from the PDF.

        Args:
            question (str): The question to answer
            max_new_tokens (int): Maximum length of generated response

        Returns:
            str: Generated answer
        """
        # Step 1: Find most relevant text chunks
        top_chunks = self._search_similar_chunks(question)
        context = "\n".join(top_chunks) # Combine chunks into single context string
        detect_lang = self._detect_language(context[:500])

        # Multilingual instruction logic
        if detect_lang == "te":
            instruction = 'Answer in both Telugu and English.'
        elif detect_lang == "ta":
            instruction = 'Answer in both Tamil and English.'
        elif detect_lang == "hi":
            instruction = 'Answer in both Hindi and English.'
        else:
            instruction = 'Answer in English'
        # Step 2: Create structured prompt for Gemini
        prompt = (
            f"You are a multilingual AI tutor. Based on the following context, answer the question clearly.\n\n"
            f"Context:\n{context}\n\n" # Relavents Passages from pdf
            f"Question:\n{question}\n\n" # Users orginal question
            f"{instruction}" # Instruction for the model
        )
        # Step 3: Generate and return response
        response = self.model.generate_content(prompt)
        return response.text.strip() # Clean up whitespaces in response


if __name__ == "__main__":
    # path of pdf
    pdf_path = "/content/drive/MyDrive/AI_Tutor/AI_Tutor/TELUGU.pdf"
    bot = PDFQuestionAnswering(pdf_path) # Initialize bot with pdf

    # Interactive Q & A loop
    while True:
        question = input("\nYour question (or 'exit'): ")
        if question.lower() == "exit":
            print("Goodbye!")
            break
        answer = bot.generate_answer(question) # Generate Answer
        print("\nðŸ¤– Answer:", answer) # Display formatted answer

# from google.colab import drive
# drive.mount('/content/drive')
